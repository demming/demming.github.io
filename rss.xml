<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Nick Demming's blog</title>
        <link>https://demming.github.io</link>
        <description><![CDATA[Musings on mathematics, Haskell, numerical analysis, data science, machine learning, and more. Stay tuned!]]></description>
        <atom:link href="https://demming.github.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Tue, 03 Mar 2020 00:00:00 UT</lastBuildDate>
        <item>
    <title>On Mathematical Modeling and Simulations for Solving Real-World Problems</title>
    <link>localhost/posts/2020-03-03-mathematical-models-solve-problems.html</link>
    <description><![CDATA[<div class="info">
    Posted on March  3, 2020
    
</div>

<hr />
<ul>
<li>[ ] TODO [2020-03-03 20:39]: accompanying flow chart</li>
</ul>
]]></description>
    <pubDate>Tue, 03 Mar 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-03-03-mathematical-models-solve-problems.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>2020-03-01-model-problem</title>
    <link>localhost/posts/2020-03-01-model-problem.html</link>
    <description><![CDATA[<div class="info">
    Posted on March  1, 2020
    
</div>


]]></description>
    <pubDate>Sun, 01 Mar 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-03-01-model-problem.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>Numerical Methods with Haskell and Python</title>
    <link>localhost/posts/2020-03-01-model-engineering-problem.html</link>
    <description><![CDATA[<div class="info">
    Posted on March  1, 2020
    
</div>

<h1 id="model-engineering-problem">Model Engineering Problem</h1>
<p>As an appetizer, a simplistic real-world engineering problem. Imagine TODO harmonic oscillator …</p>
<p>A very simple numerical method for solving an <em>initial value problem</em> (IVP), the explicit Euler’s method, can be invoked to solve our problem at hand.</p>
<p>In terms of the symplectic Hamiltonian dynamics this problem can be formulated as … TODO</p>
<p>This is a very simplified nonlinear TODO model neglecting any dissipative forces such as friction.</p>
<p>The general solution is TODO</p>
<p>A well-known exact solution is … TODO</p>
<p>How would we deal with it had we not known the exact closed-form solution? Well, first of all, what do we want to learn about it? It could be, in principle,</p>
<ol type="1">
<li>the long-term behavior of the described mechanical system,</li>
<li>the estimation of certain bounds in which the solution is still reasonable,</li>
<li>the evaluation of the solution at particular points.</li>
</ol>
<p>After all, we might be wrong with our model as applied to the real-world phenomenon.</p>
<p>A large issue is the quality of the numerical solution, i.e., it’s adherence to the exact solution, if the problem is well-posed in the first place. In reality, when we don’t know the exact solution, how do we know that the numerical solution actually represents well the exact one? What if there are many wild fluctuations or any other sorts of behavior of the exact solution that may make the numerical result seem questionable? As noted, in most cases we don’t have an easily computable representation for the exact solution. So we must find a way to assert the quality of the numerical solution, in general!</p>
<p>Using this mechanical system as our epitome, we will step-by-step develop an entire family of composite numerical schemes, of whose quality we can be certain, at the same time being very efficient!</p>
<p>In this case the known exact solution will be our guiding benchmark. Regardless, we will establish theorems that guarantee the high quality of the solution and efficiency of the method. Our <strong>goal</strong> hence will be to get as close to the exact solution from the data as possible, within an acceptable margin of error, while maintaining overall computational efficiency.</p>
<p>Here’s a comparison with the benchmark. [ ] TODO: plots of several methods for the computation of the orbit.</p>
]]></description>
    <pubDate>Sun, 01 Mar 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-03-01-model-engineering-problem.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>Numerical Methods with Haskell and Python</title>
    <link>localhost/posts/2020-03-01-initial-value-problems.html</link>
    <description><![CDATA[<div class="info">
    Posted on March  1, 2020
    
</div>

<h1 id="differential-equations">Differential Equations</h1>
<p>A general note on differential equations. There are three classical approaches to differential equations:</p>
<ol type="1">
<li>geometric or qualitative for the study of long-term behavior of a system modeled by such an equation (mathematical physics),</li>
<li>analytic of quantitative for the solution of the equation and estimation of the solutions (functional analysis),</li>
<li>numerical for evaluation, approximation, and interpolation of solutions (numerical analysis).</li>
</ol>
<p>In practice, only a handful of differential equations admit an analytic solution, i.e., an exact closed-form solution, i.e., expressible in terms of simple functions. Thus, for actual computations with differential equations we need numerical methods. For the study of long-term behavior of the solutions, differential geometry provides us with an extraordinarily rich set of tools, which are usually used for the purpose of dimensionality reduction of the problem. All three are intertwined. <a href="...">Partial differential equations</a> (PDE) are much harder. We will constrain ourselves first to the deterministic <a href="...">ordinary differential equations</a> (ODE), as opposed to <a href="...">stochastic differential equations</a> (SDE, SPDE). More on this later.</p>
<p>We first consider only the simplest case of continuous real-valued solutions <span class="math inline">\(x : \mathbb{T} \to \mathbb{R}\)</span> defined on compact intervals <span class="math inline">\(\mathbb{T} := [a,b]\)</span> of the real line <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>An exact solution is the solution without rounding errors, otherwise it is called an approximate solution. Each initial value gives rise to another solution. The vector field <span class="math inline">\(f\)</span> defines the flow in the phase space of the problem.</p>
<h1 id="initial-value-problems">Initial-value problems</h1>
<p>As a side-note, an IVP is just an ODE with an initial-value condition such as <span class="math inline">\(x(a) = x_0\)</span> for a number <span class="math inline">\(x_0 \in \mathbb{R}\)</span>. The reason for considering such a constraint is that for the problem to be <a href="...">well-posed in the sense of Hadamard</a>, it must admit a unique solution that continuously depends on the data. And the “data” is everything that is “given” as the input to the problem. In its simplest general form, an IVP is stated as <span class="math display">\[\dot{x}(t) = f\big(t, x(t)\big), \quad x(a) = x_0\]</span> for <span class="math inline">\(t \in \mathbb{T}\)</span> and some <span class="math inline">\(x_0 \in X\)</span>, where <span class="math inline">\(\mathbb{T} := [a,b]\)</span> with <span class="math inline">\(a &lt; b\)</span> and <span class="math inline">\(a,b \in \mathbb{R}\)</span>, often <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=T\)</span>, for some <span class="math inline">\(T \in \mathbb{R}\)</span>, and where <span class="math inline">\(f : \mathbb{T} \times X \to \mathbb{R}\)</span> is “<em>suitably smooth</em>” and <span class="math inline">\(X \subseteq \mathbb{R}^\mathbb{T}\)</span> is a vector space of functions <span class="math inline">\(x : \mathbb{T} \to \mathbb{R}\)</span>. The notion of “suitable smoothness” refers here to the existence and uniqueness theorems guaranteeing continuous depends of the solution on the data, e.g., <a href="...">Picard–Lindelöf</a> theorems. We simply assume that <span class="math inline">\(f\)</span> is <em>uniformly Lipschitz</em>, which means that, there exists a number <span class="math inline">\(L &gt; 0\)</span> such that for all <span class="math inline">\(t \in \mathbb{T}\)</span> and for all <span class="math inline">\(x,y \in X\)</span>, <span class="math display">\[\left| f(t,x) - f(t,y) \right| \le L \lVert x - y\rVert,\]</span> where the norm on the right-hand side is understood.</p>
<p>There are also weaker and stronger conditions. We don’t need to be concerned with these details right now, though.</p>
<h1 id="boundary-value-problems">Boundary-value problems</h1>
<p>The distinctive characteristic of an initial-value condition is that it is taken at some point of the domain of the solution that is considered “initial”, i.e., such that all other points in the domain are ordered in sequence after it. A <a href="...">directed set</a> would suffice to capture this notion, and each interval of <span class="math inline">\(\mathbb{R}\)</span> is a directed set.</p>
<p>A boundary-value condition differs in that the values constraining the problem so as to enable the existence of a unique solution, are prescribed on the boundary of the domain of solutions, which in our simplified case are the two boundary points <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> of an interval <span class="math inline">\([a,b]\)</span>, with real <span class="math inline">\(a &lt; b\)</span>.</p>
<p>This is a more general case. Indeed, we can specify various boundary-value conditions. For example,</p>
<ol type="1">
<li>… — for the values of the solution, on both ends of the interval;</li>
<li>… — for the derivative of the solution;</li>
<li>… — mixed (value and derivative);</li>
<li>… Robin — TODO</li>
</ol>
<p>In the case of PDEs the conditions are specified as directional derivatives with respect to a normal vector. In fact, in <span class="math inline">\(\mathbb{R}^d\)</span>, a normal space at a point <span class="math inline">\(p\)</span> is the orthogonal complement of a tangent space at <span class="math inline">\(p\)</span>, which leads to a minimization problem. More on this later.</p>
<h1 id="integral-equations">Integral Equations</h1>
<p>Each IVP <span class="math inline">\(\dot{x}(t) = f\big(t,x(t)\big)\)</span> with <span class="math inline">\(x(a) = x_0\)</span> can be written in form of an integral equation <span class="math display">\[x(t) = x_0 + \int_a^b f\big(t,x(t)\big) dt.\]</span></p>
<p>By … theorem TODO, both equations are equivalent, i.e., admit the same solution.</p>
<h1 id="stochastic-differential-equations">Stochastic Differential Equations</h1>
<p>This flavor of differential equations involves a stochastic component. Indeed, a differential equation as stated above is determined completely by the vector field <span class="math inline">\(f\)</span>. The vector field <span class="math inline">\(f\)</span> gives rise to a differential geometric quantity known as the vector flow of the equation, which describes in dependence on the initial data <span class="math inline">\(x_0\)</span> the behavior of the solution. More specifically, TODO</p>
<p>Any component of this constellation can be randomized. The equation can be given a stochastic additive term, the vector flow can be made stochastic, and so on. In other words, just make any of the components to the IVP dependent on an <span class="math inline">\(\omega \in \Omega\)</span>, where <span class="math inline">\((\Omega, \mathfrak{A}, \mathsf{P})\)</span> is a probability space, and the differential equation becomes essentially a stochastic differential equation.</p>
<p>So far this is only an heuristic manipulation of syntax. To make sense, such equations have different theories in which they can be given semantics. Two major compatible approaches are the Ito and the Stratanovich calculi, the former more widespread in mathematical finance, while the latter more widespread in (mathematical) physics.</p>
<p>We will tough this topic sometime later. This requires an introduction to stochastic calculus. A wonderful blog-style exposition is given by … Gowers TODO. Have a look if you’re curious.</p>
<p>SDEs are semantically correctly stated in the integral form but are often written heuristically in the differential form. Consider for instance the Ito diffusion process that models … TODO geometric Brownian motion used often in some popular formulations of theories in mathematical finance. <span class="math display">\[TODO\]</span></p>
<p>SDEs are used to model real-world phenomena where the influence of certain factors remains unknown, cannot be captured in detail, and is assumed to be “random”, whatever this word may mean. The idea is that the random influence is considered to be “noise” and due to lack of information on the precise evolution of such behavior, we simply average out the fluctuations, i.e., we take them into account but, depending on the admissible probability model, we average them out (by integrating against the model measure). This is a good approach in general, justified whenever we have no access to more detailed information or the details observable would make the problem too complex to solve in reasonable time (intractable). Much more on this later.</p>
<h1 id="applications-of-differential-equations">Applications of Differential Equations</h1>
<p>The theory of differential equations is widely applied throughout all fields of science. They are used to model real-world phenomena in terms of the rates of change of the quantities of interest in a given scientific problem. Such quantities are modeled as the so-called “features” in machine learning.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Given a set of data points from a series of observations, a scientist tries to establish equational relations between the studied quantities that are deemed to explain their rates of change, where the quantities may depend on each other and even reflexively on themselves, maybe at different points in time.</p>
<p>In a next step, the researcher needs to verify the model. Does it conform to the data? In-sample and out-of-sample data. In machine learning, a family of models is given, and the task of choosing the model is reduced to automatically fitting the data. In conventional frequentist statistics, a model is a family of probability distributions or maybe some more general measures, depending on a parameter <span class="math inline">\(\vartheta\)</span> is given, and the task is to determine the specific distribution, i.e., the specific value, range of values of the parameter or a statistic, such that the result best fits the data. In Bayesian statistics, we are given a prior distribution and … TODO, the task is then to optimize such that … TODO. In fact, the maximum likelihood estimator amounts to an optimization problem by definition.</p>
<p>In many cases of Bayesian statistics, for instance, the computation of the integral in the denominator that is used to marginalize the joint density, is a hard problem. One therefore uses alternative methods, such as the Monte Carlo sampling methods, which amount to simulations of the distribution. And even though we can compute such integrals efficiently numerically, the problem becomes intractable as soon as we have to do this for too large a set of points! So simulation may be in fact our only hope in such a case.</p>
<p>In general, simulations are a very significant tool serving as an indicator for the consistency of the assumptions. If the simulated behavior fits well with the observed, the model may be right. We will never really know, unless the system at hand is contrived. But in the scope of our findings and our human mind, this may be just enough. Certain implicit assumptions should only be made explicit by noting that this resulting model and all predictions based upon it, depend on the observed behavior, while the phenomenon of interest may exhibit different behavior under different ambient conditions, on which it intrinsically depends. In other words, there is only so much that we can observe, assert, and analyze; and there is usually no continuity to be expected, i.e., tiny shifts may make the results contrary to the prediction. A good expert will know how to deal with such limitations of predictions. This is why we need numerical, analytical, and stochastic estimates.</p>
<p>This is studied in terms of so-called hidden or latent variables in the model. A well-known model is the hidden Markov model. TODO</p>
<p>Now, my plan is to link this all together in an exposition that is aimed to elucidate the applications of pure mathematics by means of applied mathematics to real-world problems, such as</p>
<ul>
<li>the modeling of real-estate markets,</li>
<li>the modeling of financial securities,</li>
<li>the modeling of biological systems,</li>
<li>… TODO</li>
</ul>
<p>We will even write a few trading algorithms. Algotrading ahoy!</p>
<p>As I’m also interested in natural language processing (NLP), we will develop chatbots with language models based on differential equations.</p>
<p>But first of all, this should be fun!</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The term “feature” stems from the literature on pattern recognition in images: features of an object pictured in an image.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
]]></description>
    <pubDate>Sun, 01 Mar 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-03-01-initial-value-problems.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>Numerical Methods with Haskell and Python</title>
    <link>localhost/posts/2020-03-01-explicit-euler-appetizer.html</link>
    <description><![CDATA[<div class="info">
    Posted on March  1, 2020
    
</div>

<h1 id="appetizer-the-explicit-euler-method-for-the-solution-of-a-model-of-a-real-world-phenomenon">Appetizer: The Explicit Euler Method for the Solution of a Model of a Real-World Phenomenon</h1>
<h2 id="a-numerical-method-the-explicit-euler-method">A Numerical Method: the Explicit Euler Method</h2>
<p>Given such an IVP, we can solve it quite easily, albeit, generally speaking, the easier the method, the lower the quality of the solution that it yields. As usual, there is a trade-off between quality and complexity, akin the <a href="...">bias–variance trade-off</a>. The explicit Euler’s method is arguably the simplest approach to solving an IVP. We will first introduce and thereby justify the method and then provide the code. Feel free to skip any part you think you can’t comprehend instantly. Take another look at the IVP <span class="math inline">\(\dot{x}(t) = f\big(t,x(t)\big)\)</span> with <span class="math inline">\(x(0) = x_0\)</span>, and <span class="math inline">\(a=0\)</span>, <span class="math inline">\(b=1\)</span>, for brevity, <span class="math inline">\(f\)</span> uniformly Lipschitz. How do we numerically represent the quantity <span class="math inline">\(\dot{x}(t)\)</span>, for each <span class="math inline">\(t\)</span>? As we know from basic calculus, <span class="math display">\[\lim_{h \to 0} \frac{1}{h} \big( x(t+h) - x(t) \big) = \dot{x}(t) = f\big(t,x(t)\big).\]</span></p>
<p>A <a href="..">Taylor’s expansion</a> of <span class="math inline">\(x(t+h)\)</span> in <span class="math inline">\(h\)</span> about <span class="math inline">\(h=0\)</span> gives <span class="math display">\[\lim_{h \to 0} \frac{1}{h} \big( x(t+h) - x(t) \big) = \dot{x}(t) + O(h^2), \quad \text{ as } h \to 0,\]</span> where <span class="math inline">\(O(h^2)\)</span> represents terms of the order at most <span class="math inline">\(h^2\)</span>, which means that in an approximation, as <span class="math inline">\(h \to 0\)</span>, such terms vanish quadratically in <span class="math inline">\(h\)</span>, i.e., if <span class="math inline">\(h \gets h/2\)</span> then such a term <span class="math inline">\(z \gets 2^{-2}z = z/4\)</span>. And so <span class="math display">\[x(t+h) \approx x(t) + hf\big(t,x(t)\big),\]</span> where the approximate equality is understood in the quadratic sense of approximation, as <span class="math inline">\(h \to 0\)</span>.</p>
<p>Now consider a grid <span class="math inline">\(a = 0 = t_0 &lt; t_1 &lt; \dots &lt; t_n = 1 = b\)</span>, for an <span class="math inline">\(n \in \mathbb{N}\)</span>, and let <span class="math inline">\(\hat{x}_j := x(t_j)\)</span>, for <span class="math inline">\(j=0,1,\dots,n\)</span>. Then <span class="math display">\[\hat{x}_0 = x_0 \quad\text{and}\quad \frac{\hat{x}_{j+1} - \hat{x}_j}{h} = f(t_j, \hat{x}_j),\]</span> for <span class="math inline">\(j = 0,1,\dots,n-1\)</span>, that is, <span class="math display">\[\boxed{ \hat{x}_0 = x_0  \quad \text{and} \quad \hat{x}_{j+1} = \hat{x}_j + hf(t_j,\hat{x}_j)},\quad j=0,1,\dots,n-1,\]</span> which is pretty much it.</p>
<p>This method of numerical integration (to solve an ODE is equivalent to integration, more on this later) computes the slant of the direction field <span class="math inline">\(f\)</span> defined by the ODE, in every approximation point <span class="math inline">\((t_j,\hat{x}_j)\)</span> in order to determine the subsequent approximate value <span class="math inline">\(\hat{x}_{j+1}\)</span>. This is a very crude method that requires very small step size <span class="math inline">\(h\)</span> to produce an acceptable result.</p>
<h3 id="analysis-of-the-condition-of-the-problem">Analysis of the Condition of the Problem</h3>
<p>TODO</p>
<h2 id="algorithm">Algorithm</h2>
<p>TODO</p>
<h3 id="stability-rounding-error-analysis">Stability (Rounding Error) Analysis</h3>
<p>TODO</p>
<h3 id="complexity-analysis">Complexity Analysis</h3>
<p>TODO</p>
<h2 id="implementations">Implementations</h2>
<p>TODO</p>
<h3 id="c">C</h3>
<h3 id="python">Python</h3>
<h3 id="haskell">Haskell</h3>
<p>In Haskell, there are many ways to implement imperative algorithms.</p>
<ol type="1">
<li>translate into a functional-style algorithm, with recursion substituted for loops</li>
<li>sequence the pure computations in a monad, resembling the imperative paradigm</li>
<li>IORef</li>
<li>STM … TODO</li>
</ol>
<h2 id="benchmarking-implementation-performance">Benchmarking Implementation Performance</h2>
<h2 id="solving-our-model-problem">Solving our <a href="2020-03-01-model-engineering-problem.html">Model Problem</a></h2>
]]></description>
    <pubDate>Sun, 01 Mar 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-03-01-explicit-euler-appetizer.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>Numerical Methods with Haskell and Python</title>
    <link>localhost/posts/2020-02-28-numerical-haskell-python.html</link>
    <description><![CDATA[<div class="info">
    Posted on February 28, 2020
    
</div>

<h1 id="introduction">Introduction</h1>
<p>This blog post will initiate a series of tutorial posts on formulations of numerical methods of linear algebra and analysis, and perhaps stochastics, in Haskell in comparison with Python. In the process, we will also analyze the algorithms and run benchmarks to evaluate the</p>
<ul>
<li><p>performance overhead introduced by these high-level programming languages, as compared with the standard libraries,</p></li>
<li><p>performance overhead of embedding third-party numerical libraries via the FFI, subprocess calls, and bindings — with calls directly to the original numerical libraries such as LAPACK as our benchmark.</p></li>
<li><p>ease of expression of the algorithms in terms of the number of lines of code (LOC) and the mental burden, particularly in Haskell;</p>
<p>in this regard a short note: most algorithms as formulated in books are best suited for being expressed in imperative languages such as FORTRAN, C, or Python; their translation into a purely functional language is what constituted the mental burden; once you’re used to the functional paradigm, it becomes easier; the functional paradigm fits best within a functional or modular software architecture.</p></li>
</ul>
<p>Due to the complexity of this approach, I will adopt the LEAN/Agile style of publishing here, incrementally extending the posts. If all goes well, I might <strong>publish this as a book</strong> on numerical Haskell subsequently, which I was pondering on for a while now. Of course, with the usual early-bird <strong>discounts</strong>, so stay tuned!</p>
<p>I plan to introduce an interface for comments and maybe integrate it with the most popular discussion boards such as Reddit by monitoring them for references to my domain name.</p>
<p>Moreover, a conventional blog post is usually intended to bring across the major ideas in a more or less colloquial way. In fact, it is hard to write informally about formal topics. I will try to manage a good balance between the scope of exposition and depth of formal information, without unnecessarily overburdening the reader with sometimes indeed important details, whose explanation might require an extra series of posts. So I’ll just have to gloss over them but will try to remark on their importance and point to a reference material for the interested reader. Many things therein would remain subjective. Feel free to point out any incomprehensible pieces. I will try to adopt a Medium-like marker tool to reference and comment on passages, or simply cross-post over there if it is permitted by their ToC (I haven’t consulted it just yet).</p>
<p>And as a last side-note, since I speak three languages daily, I might just mix up some idioms or other stylistic idiosyncrasies. Don’t hesitate to point it out, I’m eager to improve the clarity of expression. I know how it can “brush one against the …” (German: …), unless you’re used to it.</p>
<h1 id="plan-for-this-series">Plan for This Series</h1>
<p><strong>Numerical linear algebra</strong> is the working horse of virtually all numerical methods. As most problems are reduced to a linear algebra problem. Which may seem boring to an uninitiated, efficient methods of solution of the problems of linear algebra in fact are crucial to the overall efficiency of almost all numerical methods. In fact, simple post-iteration schemes can improve the quality of a solution dramatically. Similarly, the low-quality but simple explicit Euler method can be modified into the implicit Euler method that yields a dramatically better quality of the solution, by having to solve a system of linear equations in every iteration step; so if we manage to solve this linear system efficiently, we may be able to justify the use of this more complex method of significantly greater quality!</p>
<p>In fact, it may get a bit tedious and boring to be introduced to it directly, lacking relevant motivation. So we will sparingly mix it in in pieces and justify its overarching importance to our modern world.</p>
<p>We could start outright with partial differential equations, but this may seem overwhelming.</p>
<p>Our starting point is thus the <strong>numerical solution of initial-value and boundary-value problems</strong>. We will show that <strong>they reduce to standard problems of numerical linear algebra and nonlinear optimization</strong>.</p>
<p>We will conclude this series with <strong>deep learning methods applied to ODE models</strong>.</p>
<hr />
<p>As a general note valid for all my posts here, even trying hard to stay correct, but as is usual in life, mistakes are TODO hinterhältig, they залазят в самые неожиданные места, Correctness of my words is important to me. So I will greatly appreciate if you send me your corrections. Simply pointing out a potential mistake is already greatly appreciated!</p>
]]></description>
    <pubDate>Fri, 28 Feb 2020 00:00:00 UT</pubDate>
    <guid>localhost/posts/2020-02-28-numerical-haskell-python.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>
<item>
    <title>example post</title>
    <link>localhost/posts/2015-08-23-example.html</link>
    <description><![CDATA[<div class="info">
    Posted on August 23, 2015
    
</div>

<p>Mauris in lorem nisl. Maecenas tempus facilisis ante, eget viverra nisl tincidunt et. Donec turpis lectus, mattis ac malesuada a, accumsan eu libero. Morbi condimentum, tortor et tincidunt ullamcorper, sem quam pretium nulla, id convallis lectus libero nec turpis. Proin dapibus nisi id est sodales nec ultrices tortor pellentesque.</p>
<p>Vivamus vel nisi ac lacus sollicitudin vulputate ac ut ligula. Nullam feugiat risus eget eros gravida in molestie sapien euismod. Nunc sed hendrerit orci. Nulla mollis consequat lorem ac blandit. Ut et turpis mauris. Nulla est odio, posuere id ullamcorper sit amet, tincidunt vel justo. Curabitur placerat tincidunt varius. Nulla vulputate, ipsum eu consectetur mollis, dui nibh aliquam neque, at ultricies leo ligula et arcu.</p>
]]></description>
    <pubDate>Sun, 23 Aug 2015 00:00:00 UT</pubDate>
    <guid>localhost/posts/2015-08-23-example.html</guid>
    <dc:creator>Nick Demming</dc:creator>
</item>

    </channel>
</rss>
